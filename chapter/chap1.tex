\rhead{\xiaowuhao{\sectionindex\quad绪论}}
\section{绪论}

\subsection{研究背景和意义}
在数字信息爆炸式增长的今天，搜索引擎无疑是人们获取信息资源的主要途径。根据Google提供的数据，2014年Google平均每天接收到的查询请求数已经超过了57亿次，2014年Google接收到的查询总数已经高达2万亿次 \footnote{数据来自Google Official History, ComScore}。自从搜索引擎出现以来，如何能够根据用户输入的查询词准确地捕捉到用户的查询意图，一直是搜索引擎面临的挑战和亟待解决的难题。

2012年5月，Google首次在它的搜索页面中推出了知识图谱(Knowledge Graph)，用户在使用Google进行搜索时，除了能够获得相关的网页链接以外，还将得到更加结构化的相关信息。例如，用户在输入“Barack Obama”后，谷歌不仅会提供相关的网络链接，还会在页面的右侧提供奥巴马的详细信息，包括个人简介、出生时间及地点等，具体如图\ref{knowledgegraph}所示。知识图谱的出现大大缩小了用户查找信息的范围，当用户希望获取相关信息时，不再需要自己动手访问搜索引擎返回的网页。

%%%TODO%%%
\begin{figure}[h]
  \centering
  \subfigure[]{\includegraphics[width=0.75\columnwidth]{figures/pic1.eps}}
  \caption{谷歌的知识图谱示例}
  \label{knowledgegraph}
\end{figure}

知识图谱中最重要的数据来源便是大规模的知识库，如维基百科等。知识库是由描述事实的三元组组成，其中包含了大量的结构化知识。知识库中的每个三元组将实体之间的关系表示为（头部实体，关系，尾部实体），如（中国，首都，北京）等。正因为知识库包含着大量结构化的事实性知识，其在许多人工智能系统中都有着广泛的应用，如：查询扩展\cite{}、问答系统\cite{}、文本理解\cite{}等。尽管国内外已经存在着许多大规模的知识库，如：WordNet\cite{}、Yago\cite{}、Freebase\cite{}等。但是，有限的覆盖率问题始终制约着知识库的使用与发展\cite{}。除此之外，如何在计算机中表示和存储知识库，也是知识库应用中的重要一环。

与结构化的知识库不同的是，随着Web2.0的兴起，互联网上涌现了大量的无标注文本数据。近年来，得益于深度学习\cite{}的发展，基于神经网络的表示学习方法使得计算机可以从大规模的无标注文本数据中自动学习得到文本的向量表示，并在许多自然语言处理领域的问题上取得了突破性的进展\cite{}。受到无标注文本中基于神经网络的表示方法鼓舞，研究人员开始探索面向知识库的表示学习方法，并尝试将知识库与文本进行联合式的表示学习，有效地解决了知识库的低覆盖率问题\cite{}。然而，知识库与文本的联合表示学习方法现在还比较少，而且已有的学习方法存在着一定的缺陷。因此，研究这一课题不仅有着很大的应用前景，也有着相当大的研究价值。

\subsection{国内外研究现状}
本文主要研究如何对知识库和文本的向量表示进行联合式学习，通过合适的对齐模型使得知识库中的实体向量和关系向量与文本中的词向量位于同一个向量空间中，从而进行不局限于知识库的知识推理，有效地解决知识库的低覆盖率问题。目前，国内外研究这一课题的工作还比较少，\cite{}首次提出了知识库与文本的联合表示的概念，并设计出了包含知识库模型、文本模型和对齐模型的学习框架，具体如下所示：
\begin{enumerate}
  \item 知识库模型：根据知识库中已有的三元组，学习出实体和关系的向量表示，进而可以量化给定三元组的可信度；
  \item 文本模型：根据语料库中词与词的共现情况，学习出词的向量表示，使得语义比较相关的词有着比较相似的向量表示；
  \item 对齐模型：对上述两个模型中学习得到的实体向量和词向量进行对齐，使得知识库的向量表示和文本的向量表示能够位于同一个向量空间中。
\end{enumerate}

其中，国内外研究人员对知识库模型和文本模型的研究比较深入，而且有了比较成型的研究成果和工具。相反，关于对齐模型的研究目前还比较少。本节将对此进行详细的介绍。

\subsubsection{知识库模型的研究}
近年来，随着神经网络和深度学习的革命性发展，知识库模型也受到了研究人员的关注。知识库模型指的是面向知识库的表示学习方法，其主要是通过低维度的稠密向量表征出知识库中的实体和关系的语义信息，这种表示也称为知识库嵌入（Knowledge Base Embeddings），它能够极大地简化知识库的存储和表示。

其中，Bordes等人提出的TransE模型便是最具代表性的工作。对于知识库中的任意三元组$(h,r,t)$，$h$是头部实体（head entity）、$r$是关系（relation）、$t$是尾部实体（tail entity），TransE把关系$r$的向量表示$\mathbf{r}$看作是从实体$h$的向量表示$\mathbf{h}$到实体$t$的向量表示$\mathbf{t}$的平移，并通过不断调整$\mathbf{h}$、$\mathbf{r}$和$\mathbf{t}$，使得$(\mathbf{h}+\mathbf{r})$尽可能与$\mathbf{t}$相等，即$\mathbf{h}+\mathbf{r} \approx \mathbf{t}$。通过优化上述目标，TransE能够有效地学习出实体和关系的向量表示，使得如果一个候选三元组$(h,r,t)$是有效的，那么$(\mathbf{h}+\mathbf{r})$将与$\mathbf{t}$十分相近。因此，TransE根据公式\ref{eqn:transe}对三元组$(h,r,t)$进行评分，如果$(h,r,t)$是有效的，$f_r(h,t)$将会很小。通过$f_r(h,t)$的定义，TransE能够帮助知识库进行知识推理，进而有效地缓解知识库的低覆盖率问题。\cite{}中的实验也证明了TransE有着比传统方法更优的性能，因此很多工作就此展开。
\begin{equation}
  f_r(h,t)=\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_2^2
  \label{eqn:transe}
\end{equation}

由于TransE的模型过于简单，使得它只能在处理一对一（1-to-1）这种类型的关系时取得比较好的效果，并不能很好地表示一对多（1-to-N）、多对一（N-to-1）以及多对多（N-to-N）类型的关系。例如，对于一个多对一的关系$r$，$\forall i \in \{ 1, ..., m \}$，$(h_i, r, t)$都是有效的，TransE的模型会使得$\mathbf{h_1}=...=\mathbf{h_m}$，这显然是不合理的。为了解决TransE在表示这些关系时的问题，Wang等人在2014年提出了TransH\cite{}，其主要思想是每个关系向量都会有自己特定的超平面，而每一个实体在对于不同的关系时都会有不同的向量表示。对于任意一个关系$r$，TransH不仅会学习它的向量表示$\mathbf{r}$，还会学习它所位于超平面的法向量表示$\mathbf{w_r}$。对于给定的三元组$(h,r,t)$，TransH首先会将实体向量$\mathbf{h}$和$\mathbf{t}$投影到关系$r$的超平面中，得到向量$\mathbf{h_r}$和$\mathbf{h_t}$，即$\mathbf{h_r}=\mathbf{h}-\mathbf{w_r}^\top\mathbf{h}\mathbf{w_r}$，$\mathbf{t_r}=\mathbf{t}-\mathbf{w_r}^\top\mathbf{t}\mathbf{w_r}$。因此，TransH根据公式\ref{eqn:transh}对三元组进行评分。
\begin{equation}
  f_r(h,t)=\|\mathbf{h_r}+\mathbf{r}-\mathbf{t_r}\|_2^2
  \label{eqn:transh}
\end{equation}

除此之外，Wang等人通过修改TransE的评分函数和目标函数，提出了一个新的概率表示模型pTransE\cite{}。pTransE根据公式$z(h,r,t)=b-\frac{1}{2}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_2^2$对$(h,r,t)$进行评分，并定义$\Pr(h|r,t)$作为给定关系$r$和尾部实体$t$时，头部实体是$h$的概率。类似地，我们也可以得到$\Pr(r|h,t)$和$Pr(t|h,r)$的定义，最后pTransE通过最大似然估计可以学习出实体$h$、$t$和关系$r$的向量表示。\cite{}中的实验也表明了pTransE有着比TransE更优的性能。本文主要采用pTransE作为知识库模型，因此会在后续的章节中进行更详细的阐述。

\subsubsection{文本模型的研究}
在自然语言处理领域，最常用的文本表示方法就是词袋模型\cite{}，然而这种方法存在着语义缺失和忽略词序两大缺陷，成为了许多自然语言处理系统性能提升的瓶颈。随着机器学习和深度学习的推广，基于神经网络的文本表示技术得到了空前的发展，本节讨论的文本模型指的就是基于神经网络的表示方法。根据Harris提出的的分布式假说“如果两个词的上下文相似，它们的语义也相似”\cite{}，基于神经网络的文本表示方法可以从大规模的无标注文本中自动学习得到词的向量表示，这种表示也称为词嵌入／词向量（Word Embeddings）。通过词向量，我们能够有效地捕捉到词的语义信息，可以直接刻画词与词之间的相似度。

早期，词向量只是神经网络语言模型的副产品，神经网络语言模型通过对上下文和目标词之间的关系进行建模，在学习语言模型的同时，也能有效地学习得到词向量\cite{}。简单地说，语言模型就是对一段文本进行概率估计，这对许多自然语言处理任务有着十分重要的作用，如机器翻译、语音识别等。对于一个长度为$m$的文本字符串，语言模型通过概率分布$P(w_1, w_2, ..., w_m)$表示该文本的可能性，根据链式法则，我们可以通过公式\ref{}对该概率值进行求解。
\begin{equation}
\begin{split}
  P(w_1,w_2,...,w_m) =& P(w_1) P(w_2|w_1) P(w_3|w_1,w_2) \\
                        ... P(w_i|w_1,w_2,...,w_{i-1}) ... P(w_m|w_1,w_2,...,w_{m-1})
  \label{eqn:languagemodel}
\end{split}
\end{equation}

当文本长度较长时，直接求解$P(w_i|w_1,w_2,...,w_{i-1})$会比较困难，因此研究人员提出了它的简化模型：n元模型（n-gram model）。n元模型会直接忽略与目标词距离大于等于n的上下文词，即$P(w_i|w_1,w_2,...,w_{i-1}) \approx P(w_i|w_{i-(n-1)},...,w_{i-1})$。在求解n元模型中的条件概率时，传统的方法直接通过统计n元出现次数的方式进行计算，即：
\begin{equation}
  P(w_i|w_{i-(n-1)},...,w_{i-1})=\frac{Count(w_{i-(n-1)},...,w_{i})}{Count(w_{i-(n-1)},...,w_{i-1})}
\end{equation}

然而，这种方法在求解n元模型时会严重受到数据稀疏带来的影响，因此Bengio等人在2001年提出了基于神经网络的语言模型（NNLM）\cite{}来解决这一问题。不同于传统方法，NNLM直接通过神经网络对n元模型中的条件概率进行求解，该模型在学习语言模型的同时，也将得到词向量。NNLM的出现使得词向量的研究得到了空前的发展，其中最为经典和普遍使用的便是Google公开的word2vec工具\footnote{}。word2vec中主要提出了连续词袋模型（CBOW）和跳元文法模型（Skip-gram）\cite{}，基于简单的三层神经网络，并通过滑动窗口的输入对词向量的表示进行学习,这两种模型都能高效地从大规模的文本语料中学习得到质量较高的词向量。本文主要采用Skip-gram作为文本模型，因此会在后续的章节中进行更详细的介绍。

受到Skip-gram的影响，Le等人尝试使用了类似的神经网络结构直接对篇章／段落进行建模：根据篇章与词的共现情况学习出篇章的向量表示（Paragraph Vector）\cite{}。其实验结果表明，篇章向量能够有效地捕捉到篇章的语义信息，并在文本分类和情感分析等任务上取得了当时最优的性能。本文在学习知识库和文本的联合表示时，利用实体的描述文本作为实体向量与词向量的连接桥梁，采用篇章向量对实体的描述文本进行建模，并根据其在实验中的不足进行改进，因此会在后续的章节中对篇章向量进行更详细的介绍。

\subsubsection{对齐模型的研究}
对齐模型主要是将

\subsection{论文内容和结构}